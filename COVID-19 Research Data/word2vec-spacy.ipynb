{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import utils\n",
    "import spacy\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tasks import *\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles retrieved from biorxiv: 885\n"
     ]
    }
   ],
   "source": [
    "biorxiv_dir = 'data/biorxiv_medrxiv/biorxiv_medrxiv/'\n",
    "filenames = os.listdir(biorxiv_dir)\n",
    "print(\"Number of articles retrieved from biorxiv:\", len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for filename in filenames:\n",
    "    filename = biorxiv_dir + filename\n",
    "    file = json.load(open(filename, 'rb'))\n",
    "    all_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a693b6d7fcc49389f0deb017e734313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=885.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_files = []\n",
    "\n",
    "for file in tqdm(all_files):\n",
    "    features = [\n",
    "        file['paper_id'],\n",
    "        file['metadata']['title'],\n",
    "        utils.format_authors(file['metadata']['authors']),\n",
    "        utils.format_authors(file['metadata']['authors'], \n",
    "                       with_affiliation = True),\n",
    "        utils.format_body(file['abstract']),\n",
    "        utils.format_body(file['body_text']),\n",
    "        utils.format_bib(file['bib_entries']),\n",
    "        file['metadata']['authors'],\n",
    "        file['bib_entries']\n",
    "    ]\n",
    "    \n",
    "    cleaned_files.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'paper_id', \n",
    "    'title', \n",
    "    'authors',\n",
    "    'affiliations', \n",
    "    'abstract', \n",
    "    'text', \n",
    "    'bibliography',\n",
    "    'raw_authors',\n",
    "    'raw_bibliography'\n",
    "]\n",
    "\n",
    "clean_df = pd.DataFrame(cleaned_files, columns = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16959/16959 [04:45<00:00, 59.48it/s]\n",
      "100%|██████████| 16959/16959 [00:40<00:00, 418.10it/s]\n"
     ]
    }
   ],
   "source": [
    "pmc_dir = 'data/custom_license/custom_license/'\n",
    "pmc_files = utils.load_files(pmc_dir)\n",
    "pmc_df = utils.generate_clean_df(pmc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9118/9118 [01:39<00:00, 91.85it/s] \n",
      "100%|██████████| 9118/9118 [00:24<00:00, 371.86it/s]\n"
     ]
    }
   ],
   "source": [
    "comm_dir = 'data/comm_use_subset/comm_use_subset/'\n",
    "comm_files = utils.load_files(comm_dir)\n",
    "comm_df = utils.generate_clean_df(comm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2353/2353 [00:30<00:00, 76.11it/s] \n",
      "100%|██████████| 2353/2353 [00:04<00:00, 483.77it/s]\n"
     ]
    }
   ],
   "source": [
    "noncomm_dir = 'data/noncomm_use_subset/noncomm_use_subset/'\n",
    "noncomm_files = utils.load_files(noncomm_dir)\n",
    "noncomm_df = utils.generate_clean_df(noncomm_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29315, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df = pd.concat([clean_df, pmc_df, comm_df, noncomm_df])\n",
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27139, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df = complete_df[complete_df['text'].apply(lambda x: len(re.findall(r\"(?i)\\b[a-z]+\\b\", x))) > 1000]                                           \n",
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_of_articles = 1\n",
    "train_df  = complete_df.sample(frac = frac_of_articles, random_state = 42)\n",
    "train_corpus = [i.split() for i in train_df[\"abstract\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16973856, 23391565)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(min_count = 10, seed = 42, workers = 6)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples = model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tasks = [task_1, task_2, task_3, task_4, task_5, task_6, task_7, task_8, task_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"data/word2vec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data for spacy model\n",
    "!gzip data/word2vec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Successfully created model\u001b[0m\n",
      "24973it [00:01, 19213.88it/s]ord2vec.txt.gz\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded vectors from data/word2vec.txt.gz\u001b[0m\n",
      "\u001b[38;5;2m✔ Sucessfully compiled vocab\u001b[0m\n",
      "25351 entries, 24973 vectors\n"
     ]
    }
   ],
   "source": [
    "# Init spacy model\n",
    "!python -m spacy init-model en models/spacy.word2vec.model --vectors-loc data/word2vec.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "nlp = spacy.load('models/spacy.word2vec.model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
