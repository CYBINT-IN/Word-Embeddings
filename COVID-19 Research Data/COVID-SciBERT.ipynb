{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all special charachters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicPreprocess(text):\n",
    "    processed_text = text.lower()\n",
    "    processed_text = re.sub(r\"[^a-zA-Z0-9]+\", ' ', processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is based on the [CORD Challenge](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) on Kaggle, to check out how I cleaned it up from source and converted it into a CSV for ease of use, please check out [my notebook](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/prep_pdf.ipynb).\n",
    "\n",
    "[Download this CSV](https://drive.google.com/file/d/1n6r40XFGlYF9phWP-Hx6Y4QiTqw_I7uS/view?usp=sharing) for yourself. It's approximately 4 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.read_csv(\"data/clean_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete_df.sample(frac = 1).sample(frac = 1)\n",
    "data.dropna(inplace = True)\n",
    "del complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\"abstract\"].apply(basicPreprocess).replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for i in data.values:\n",
    "    text += i\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(text.split())\n",
    "del text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove words that are too frequent or infrequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for keys, values in counter.items():\n",
    "    if(values > 100 and values < 10000):\n",
    "        vocab.append(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6735"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Load Pretrained model and expand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the awesome [Allen AI SciBERT Model](https://github.com/allenai/scibert) which is a BERT model trained on scientific text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = transformers.AutoModelWithLMHead.from_pretrained('allenai/scibert_scivocab_uncased').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 31090\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31090\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add new tokens to the existing tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31941\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens(vocab)\n",
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to resize the dictionary size of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 31941\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fine Tune Model for Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/COVID-scibert-latest/vocab.txt',\n",
       " 'models/COVID-scibert-latest/special_tokens_map.json',\n",
       " 'models/COVID-scibert-latest/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.mkdir('models/COVID-scibert-latest')\n",
    "tokenizer.save_pretrained('models/COVID-scibert-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = transformers.LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = \"lm_data/train.txt\",\n",
    "    block_size = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer, mlm = True, mlm_probability = 0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir = \"models/COVID-scibert-latest\",\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    save_steps = 10_000,\n",
    "    save_total_limit = 3,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = dataset,\n",
    "    prediction_loss_only = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312632d3b061481e9b809a5416bceb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2d09ba14ab4cebb085255c62191830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2259.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.9176381278038024, \"learning_rate\": 4.778663125276671e-05, \"epoch\": 0.2213368747233289, \"step\": 500}\n",
      "{\"loss\": 2.6581398136615753, \"learning_rate\": 4.557326250553343e-05, \"epoch\": 0.4426737494466578, \"step\": 1000}\n",
      "{\"loss\": 2.5586014343500136, \"learning_rate\": 4.335989375830013e-05, \"epoch\": 0.6640106241699867, \"step\": 1500}\n",
      "{\"loss\": 2.436116787314415, \"learning_rate\": 4.114652501106684e-05, \"epoch\": 0.8853474988933157, \"step\": 2000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8228ac5051df4f23813b4eb8d1132aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2259.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.3573547369241714, \"learning_rate\": 3.893315626383356e-05, \"epoch\": 1.1066843736166445, \"step\": 2500}\n",
      "{\"loss\": 2.2459749839305876, \"learning_rate\": 3.671978751660027e-05, \"epoch\": 1.3280212483399734, \"step\": 3000}\n",
      "{\"loss\": 2.218323789358139, \"learning_rate\": 3.450641876936698e-05, \"epoch\": 1.5493581230633025, \"step\": 3500}\n",
      "{\"loss\": 2.2082864822149277, \"learning_rate\": 3.229305002213369e-05, \"epoch\": 1.7706949977866313, \"step\": 4000}\n",
      "{\"loss\": 2.141116299211979, \"learning_rate\": 3.00796812749004e-05, \"epoch\": 1.9920318725099602, \"step\": 4500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c74cc30aec14d9986197f2683f505ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2259.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.041673380970955, \"learning_rate\": 2.786631252766711e-05, \"epoch\": 2.213368747233289, \"step\": 5000}\n",
      "{\"loss\": 2.007178209066391, \"learning_rate\": 2.565294378043382e-05, \"epoch\": 2.434705621956618, \"step\": 5500}\n",
      "{\"loss\": 2.0318748190402984, \"learning_rate\": 2.3439575033200534e-05, \"epoch\": 2.6560424966799467, \"step\": 6000}\n",
      "{\"loss\": 1.9773010560274125, \"learning_rate\": 2.1226206285967244e-05, \"epoch\": 2.8773793714032756, \"step\": 6500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9c0d87295e4f25976a5ec255644481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2259.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.9445158489942551, \"learning_rate\": 1.9012837538733954e-05, \"epoch\": 3.098716246126605, \"step\": 7000}\n",
      "{\"loss\": 1.8315122603178025, \"learning_rate\": 1.6799468791500664e-05, \"epoch\": 3.3200531208499338, \"step\": 7500}\n",
      "{\"loss\": 1.8183673046827316, \"learning_rate\": 1.4586100044267376e-05, \"epoch\": 3.5413899955732626, \"step\": 8000}\n",
      "{\"loss\": 1.8523061389923097, \"learning_rate\": 1.2372731297034086e-05, \"epoch\": 3.7627268702965915, \"step\": 8500}\n",
      "{\"loss\": 1.8157472529411316, \"learning_rate\": 1.0159362549800798e-05, \"epoch\": 3.9840637450199203, \"step\": 9000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bab73a0baf42f891bd91fe1ae5dc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=2259.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.7939044281244279, \"learning_rate\": 7.945993802567508e-06, \"epoch\": 4.20540061974325, \"step\": 9500}\n",
      "{\"loss\": 1.765317411363125, \"learning_rate\": 5.732625055334219e-06, \"epoch\": 4.426737494466578, \"step\": 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/anaconda3/envs/torch/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 1.7384810733795166, \"learning_rate\": 3.51925630810093e-06, \"epoch\": 4.648074369189907, \"step\": 10500}\n",
      "{\"loss\": 1.7090454839468003, \"learning_rate\": 1.3058875608676407e-06, \"epoch\": 4.869411243913236, \"step\": 11000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11295, training_loss=2.084379817630507)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/COVID-scibert-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Pipeline the Model for mask filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelWithLMHead.from_pretrained('models/COVID-scibert-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"models/COVID-scibert-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] coronavirus or covid - 19 can be prevented by a combination [SEP]',\n",
       "  'score': 0.1719885915517807,\n",
       "  'token': 2702},\n",
       " {'sequence': '[CLS] coronavirus or covid - 19 can be prevented by a simple [SEP]',\n",
       "  'score': 0.054218728095293045,\n",
       "  'token': 2177},\n",
       " {'sequence': '[CLS] coronavirus or covid - 19 can be prevented by a novel [SEP]',\n",
       "  'score': 0.043364267796278,\n",
       "  'token': 3045},\n",
       " {'sequence': '[CLS] coronavirus or covid - 19 can be prevented by a high [SEP]',\n",
       "  'score': 0.03732519596815109,\n",
       "  'token': 597},\n",
       " {'sequence': '[CLS] coronavirus or covid - 19 can be prevented by a vaccine [SEP]',\n",
       "  'score': 0.021863549947738647,\n",
       "  'token': 7039}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill = transformers.pipeline('fill-mask', model = model, tokenizer = tokenizer)\n",
    "nlp_fill('Coronavirus or COVID-19 can be prevented by a' + nlp_fill.tokenizer.mask_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
